{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2cd4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vizdoom as vzd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools as it\n",
    "import skimage.transform\n",
    "\n",
    "from vizdoom import Mode\n",
    "from time import sleep, time\n",
    "from collections import deque\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fc8f737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n",
      "Initializing new model\n",
      "\n",
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: -76.2 +/- 201.6, min: -395.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 0.4 +/- 144.3, min: -405.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 5.92 minutes\n",
      "\n",
      "Epoch #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 45.4 +/- 106.6, min: -410.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 62.9 +/- 91.8, min: -405.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 11.80 minutes\n",
      "\n",
      "Epoch #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 76.3 +/- 51.8, min: -400.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 87.0 +/- 13.0, min: 38.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 17.47 minutes\n",
      "\n",
      "Epoch #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 81.5 +/- 33.9, min: -375.0, max: 95.0,\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: mean: 80.1 +/- 42.0, min: -226.0 max: 95.0\n",
      "Saving the network weights to: ./model-doom.pth\n",
      "Total elapsed time: 22.81 minutes\n",
      "\n",
      "Epoch #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5884/3183881950.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    278\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mskip_learning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m         agent, game = run(game, agent, actions, num_epochs=train_epochs, frame_repeat=frame_repeat,\n\u001b[1;32m--> 280\u001b[1;33m                           steps_per_epoch=learning_steps_per_epoch)\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"======================================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5884/3183881950.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(game, agent, actions, num_epochs, frame_repeat, steps_per_epoch)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00025\n",
    "discount_factor = 0.99\n",
    "train_epochs = 5\n",
    "learning_steps_per_epoch = 2000\n",
    "replay_memory_size = 10000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "test_episodes_per_epoch = 100\n",
    "\n",
    "frame_repeat = 12\n",
    "resolution = (30, 45)\n",
    "episodes_to_watch = 10\n",
    "\n",
    "model_savefile = \"./model-doom.pth\"\n",
    "save_model = True\n",
    "load_model = False\n",
    "skip_learning = False\n",
    "\n",
    "\n",
    "config_file_path = \"vizdoom/scenarios/simpler_basic.cfg\"\n",
    "# config_file_path = \"../../scenarios/rocket_basic.cfg\"\n",
    "# config_file_path = \"../../scenarios/basic.cfg\"\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"Down samples image to resolution\"\"\"\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def create_simple_game():\n",
    "    print(\"Initializing doom...\")\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(config_file_path)\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "\n",
    "    return game\n",
    "\n",
    "\n",
    "def test(game, agent):\n",
    "    \"\"\"Runs a test_episodes_per_epoch episodes and prints the result\"\"\"\n",
    "    print(\"\\nTesting...\")\n",
    "    test_scores = []\n",
    "    for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.make_action(actions[best_action_index], frame_repeat)\n",
    "        r = game.get_total_reward()\n",
    "        test_scores.append(r)\n",
    "\n",
    "    test_scores = np.array(test_scores)\n",
    "    print(\"Results: mean: %.1f +/- %.1f,\" % (\n",
    "        test_scores.mean(), test_scores.std()), \"min: %.1f\" % test_scores.min(),\n",
    "          \"max: %.1f\" % test_scores.max())\n",
    "\n",
    "\n",
    "def run(game, agent, actions, num_epochs, frame_repeat, steps_per_epoch=2000):\n",
    "    \"\"\"\n",
    "    Run num epochs of training episodes.\n",
    "    Skip frame_repeat number of frames after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        game.new_episode()\n",
    "        train_scores = []\n",
    "        global_step = 0\n",
    "        print(\"\\nEpoch #\" + str(epoch + 1))\n",
    "\n",
    "        for _ in trange(steps_per_epoch, leave=False):\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            action = agent.get_action(state)\n",
    "            reward = game.make_action(actions[action], frame_repeat)\n",
    "            done = game.is_episode_finished()\n",
    "\n",
    "            if not done:\n",
    "                next_state = preprocess(game.get_state().screen_buffer)\n",
    "            else:\n",
    "                next_state = np.zeros((1, 30, 45)).astype(np.float32)\n",
    "\n",
    "            agent.append_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            if global_step > agent.batch_size:\n",
    "                agent.train()\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(game.get_total_reward())\n",
    "                game.new_episode()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        agent.update_target_net()\n",
    "        train_scores = np.array(train_scores)\n",
    "\n",
    "        print(\"Results: mean: %.1f +/- %.1f,\" % (train_scores.mean(), train_scores.std()),\n",
    "              \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())\n",
    "\n",
    "        test(game, agent)\n",
    "        if save_model:\n",
    "            print(\"Saving the network weights to:\", model_savefile)\n",
    "            torch.save(agent.q_net, model_savefile)\n",
    "        print(\"Total elapsed time: %.2f minutes\" % ((time() - start_time) / 60.0))\n",
    "\n",
    "    game.close()\n",
    "    return agent, game\n",
    "\n",
    "\n",
    "class DuelQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This is Duel DQN architecture.\n",
    "    see https://arxiv.org/abs/1511.06581 for more information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, available_actions_count):\n",
    "        super(DuelQNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.state_fc = nn.Sequential(\n",
    "            nn.Linear(96, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(96, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, available_actions_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(-1, 192)\n",
    "        x1 = x[:, :96]  # input for the net to calculate the state value\n",
    "        x2 = x[:, 96:]  # relative advantage of actions in the state\n",
    "        state_value = self.state_fc(x1).reshape(-1, 1)\n",
    "        advantage_values = self.advantage_fc(x2)\n",
    "        x = state_value + (advantage_values - advantage_values.mean(dim=1).reshape(-1, 1))\n",
    "\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, memory_size, batch_size, discount_factor, \n",
    "                 lr, load_model, epsilon=1, epsilon_decay=0.9996, epsilon_min=0.1):\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount_factor\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        if load_model:\n",
    "            print(\"Loading model from: \", model_savefile)\n",
    "            self.q_net = torch.load(model_savefile)\n",
    "            self.target_net = torch.load(model_savefile)\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing new model\")\n",
    "            self.q_net = DuelQNet(action_size).to(DEVICE)\n",
    "            self.target_net = DuelQNet(action_size).to(DEVICE)\n",
    "\n",
    "        self.opt = optim.SGD(self.q_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            state = torch.from_numpy(state).float().to(DEVICE)\n",
    "            action = torch.argmax(self.q_net(state)).item()\n",
    "            return action\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        batch = np.array(batch, dtype=object)\n",
    "\n",
    "        states = np.stack(batch[:, 0]).astype(float)\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2].astype(float)\n",
    "        next_states = np.stack(batch[:, 3]).astype(float)\n",
    "        dones = batch[:, 4].astype(bool)\n",
    "        not_dones = ~dones\n",
    "\n",
    "        row_idx = np.arange(self.batch_size)  # used for indexing the batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "            idx = row_idx, np.argmax(self.q_net(next_states).cpu().data.numpy(), 1)\n",
    "            next_state_values = self.target_net(next_states).cpu().data.numpy()[idx]\n",
    "            next_state_values = next_state_values[not_dones]\n",
    "\n",
    "        \n",
    "        q_targets = rewards.copy()\n",
    "        q_targets[not_dones] += self.discount * next_state_values\n",
    "        q_targets = torch.from_numpy(q_targets).float().to(DEVICE)\n",
    "\n",
    "        \n",
    "        idx = row_idx, actions\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "        action_values = self.q_net(states)[idx].float().to(DEVICE)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        td_error = self.criterion(q_targets, action_values)\n",
    "        td_error.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    game = create_simple_game()\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "    \n",
    "    agent = DQNAgent(len(actions), lr=learning_rate, batch_size=batch_size,\n",
    "                     memory_size=replay_memory_size, discount_factor=discount_factor,\n",
    "                     load_model=load_model)\n",
    "\n",
    "    \n",
    "    if not skip_learning:\n",
    "        agent, game = run(game, agent, actions, num_epochs=train_epochs, frame_repeat=frame_repeat,\n",
    "                          steps_per_epoch=learning_steps_per_epoch)\n",
    "\n",
    "        print(\"======================================\")\n",
    "        print(\"Training finished. It's time to watch!\")\n",
    "\n",
    "    \n",
    "    game.close()\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(Mode.ASYNC_PLAYER)\n",
    "    game.init()\n",
    "\n",
    "    for _ in range(episodes_to_watch):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "           \n",
    "            game.set_action(actions[best_action_index])\n",
    "            for _ in range(frame_repeat):\n",
    "                game.advance_action()\n",
    "\n",
    "       \n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14c2f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
