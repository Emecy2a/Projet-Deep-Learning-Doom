{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b1006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vizdoom as vzd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import itertools as it\n",
    "import skimage.transform\n",
    "\n",
    "from vizdoom import Mode\n",
    "from time import sleep, time\n",
    "from collections import deque\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c7c62bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing doom...\n",
      "Doom initialized.\n",
      "Initializing new model\n",
      "\n",
      "Epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "ename": "ViZDoomUnexpectedExitException",
     "evalue": "Controlled ViZDoom instance exited unexpectedly.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11056/541829820.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mskip_learning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         agent, game = run(game, agent, actions, num_epochs=train_epochs, frame_repeat=frame_repeat,\n\u001b[1;32m--> 300\u001b[1;33m                           steps_per_epoch=learning_steps_per_epoch)\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"======================================\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11056/541829820.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(game, agent, actions, num_epochs, frame_repeat, steps_per_epoch)\u001b[0m\n\u001b[0;32m    108\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m             \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_episode_finished\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mViZDoomUnexpectedExitException\u001b[0m: Controlled ViZDoom instance exited unexpectedly."
     ]
    }
   ],
   "source": [
    "#Apprentissage\n",
    "learning_rate = 0.00025\n",
    "discount_factor = 0.99\n",
    "train_epochs = 5\n",
    "learning_steps_per_epoch = 2000\n",
    "replay_memory_size = 10000\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "test_episodes_per_epoch = 100\n",
    "\n",
    "# Paramètres traitement d'image\n",
    "frame_repeat = 12\n",
    "resolution = (30, 45)\n",
    "episodes_to_watch = 10\n",
    "\n",
    "#Sauvegarde de l'apprentissage\n",
    "model_savefile = \"./complex-doom.pth\"\n",
    "save_model = True\n",
    "load_model = False\n",
    "skip_learning = False\n",
    "\n",
    "# Scénario choisi\n",
    "config_file_path = \"vizdoom/scenarios/deadly_corridor.cfg\"\n",
    "#config_file_path = \"vizdoom/scenarios/simpler_basic.cfg\"\n",
    "\n",
    "\n",
    "# CUDA\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "\n",
    "def preprocess(img):\n",
    "    \"\"\"Down samples image to resolution\"\"\"\n",
    "    img = skimage.transform.resize(img, resolution)\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "#Lancement et affichage de Doom\n",
    "def create_simple_game():\n",
    "    print(\"Initializing doom...\")\n",
    "    game = vzd.DoomGame()\n",
    "    game.load_config(config_file_path)\n",
    "    game.set_window_visible(True)\n",
    "    game.set_mode(Mode.PLAYER)\n",
    "    game.set_screen_format(vzd.ScreenFormat.GRAY8)\n",
    "    game.set_screen_resolution(vzd.ScreenResolution.RES_640X480)\n",
    "    game.init()\n",
    "    print(\"Doom initialized.\")\n",
    "\n",
    "    return game\n",
    "\n",
    "\n",
    "def test(game, agent):\n",
    "    \"\"\"Runs a test_episodes_per_epoch episodes and prints the result\"\"\"\n",
    "    print(\"\\nTesting...\")\n",
    "    test_scores = []\n",
    "    for test_episode in trange(test_episodes_per_epoch, leave=False):\n",
    "        game.new_episode()\n",
    "        while not game.is_episode_finished():\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            best_action_index = agent.get_action(state)\n",
    "\n",
    "            game.make_action(actions[best_action_index], frame_repeat)\n",
    "        r = game.get_total_reward()\n",
    "        test_scores.append(r)\n",
    "\n",
    "    test_scores = np.array(test_scores)\n",
    "    print(\"Results: mean: %.1f +/- %.1f,\" % (\n",
    "        test_scores.mean(), test_scores.std()), \"min: %.1f\" % test_scores.min(),\n",
    "          \"max: %.1f\" % test_scores.max())\n",
    "\n",
    "\n",
    "def run(game, agent, actions, num_epochs, frame_repeat, steps_per_epoch=2000):\n",
    "    \"\"\"\n",
    "    Run num epochs of training episodes.\n",
    "    Skip frame_repeat number of frames after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        game.new_episode()\n",
    "        train_scores = []\n",
    "        global_step = 0\n",
    "        print(\"\\nEpoch #\" + str(epoch + 1))\n",
    "\n",
    "        for _ in trange(steps_per_epoch, leave=False):\n",
    "            state = preprocess(game.get_state().screen_buffer)\n",
    "            action = agent.get_action(state)\n",
    "            reward = game.make_action(actions[action], frame_repeat)\n",
    "            done = game.is_episode_finished()\n",
    "\n",
    "            if not done:\n",
    "                next_state = preprocess(game.get_state().screen_buffer)\n",
    "            else:\n",
    "                next_state = np.zeros((1, 30, 45)).astype(np.float32)\n",
    "\n",
    "            agent.append_memory(state, action, reward, next_state, done)\n",
    "\n",
    "            if global_step > agent.batch_size:\n",
    "                agent.train()\n",
    "\n",
    "            if done:\n",
    "                train_scores.append(game.get_total_reward())\n",
    "                game.new_episode()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        agent.update_target_net()\n",
    "        train_scores = np.array(train_scores)\n",
    "\n",
    "        print(\"Results: mean: %.1f +/- %.1f,\" % (train_scores.mean(), train_scores.std()),\n",
    "              \"min: %.1f,\" % train_scores.min(), \"max: %.1f,\" % train_scores.max())\n",
    "\n",
    "        test(game, agent)\n",
    "        if save_model:\n",
    "            print(\"Saving the network weights to:\", model_savefile)\n",
    "            torch.save(agent.q_net, model_savefile)\n",
    "        print(\"Total elapsed time: %.2f minutes\" % ((time() - start_time) / 60.0))\n",
    "\n",
    "    game.close()\n",
    "    return agent, game\n",
    "\n",
    "#Architecture d'apprentissage\n",
    "class DuelQNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This is Duel DQN architecture.\n",
    "    see https://arxiv.org/abs/1511.06581 for more information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, available_actions_count):\n",
    "        super(DuelQNet, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.state_fc = nn.Sequential(\n",
    "            nn.Linear(96, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        self.advantage_fc = nn.Sequential(\n",
    "            nn.Linear(96, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, available_actions_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(-1, 192)\n",
    "        x1 = x[:, :96]  # input for the net to calculate the state value\n",
    "        x2 = x[:, 96:]  # relative advantage of actions in the state\n",
    "        state_value = self.state_fc(x1).reshape(-1, 1)\n",
    "        advantage_values = self.advantage_fc(x2)\n",
    "        x = state_value + (advantage_values - advantage_values.mean(dim=1).reshape(-1, 1))\n",
    "\n",
    "        return x\n",
    "#Agent d'apprentissage\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size, memory_size, batch_size, discount_factor, \n",
    "                 lr, load_model, epsilon=1, epsilon_decay=0.9996, epsilon_min=0.1):\n",
    "        self.action_size = action_size\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = discount_factor\n",
    "        self.lr = lr\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        if load_model:\n",
    "            print(\"Loading model from: \", model_savefile)\n",
    "            self.q_net = torch.load(model_savefile)\n",
    "            self.target_net = torch.load(model_savefile)\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "        else:\n",
    "            print(\"Initializing new model\")\n",
    "            self.q_net = DuelQNet(action_size).to(DEVICE)\n",
    "            self.target_net = DuelQNet(action_size).to(DEVICE)\n",
    "\n",
    "        self.opt = optim.SGD(self.q_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0)\n",
    "            state = torch.from_numpy(state).float().to(DEVICE)\n",
    "            action = torch.argmax(self.q_net(state)).item()\n",
    "            return action\n",
    "\n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "    def append_memory(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        batch = np.array(batch, dtype=object)\n",
    "\n",
    "        states = np.stack(batch[:, 0]).astype(float)\n",
    "        actions = batch[:, 1].astype(int)\n",
    "        rewards = batch[:, 2].astype(float)\n",
    "        next_states = np.stack(batch[:, 3]).astype(float)\n",
    "        dones = batch[:, 4].astype(bool)\n",
    "        not_dones = ~dones\n",
    "\n",
    "        row_idx = np.arange(self.batch_size)  # used for indexing the batch\n",
    "\n",
    "        # value of the next states with double q learning\n",
    "        # see https://arxiv.org/abs/1509.06461 for more information on double q learning\n",
    "        with torch.no_grad():\n",
    "            next_states = torch.from_numpy(next_states).float().to(DEVICE)\n",
    "            idx = row_idx, np.argmax(self.q_net(next_states).cpu().data.numpy(), 1)\n",
    "            next_state_values = self.target_net(next_states).cpu().data.numpy()[idx]\n",
    "            next_state_values = next_state_values[not_dones]\n",
    "\n",
    "        # this defines y = r + discount * max_a q(s', a)\n",
    "        q_targets = rewards.copy()\n",
    "        q_targets[not_dones] += self.discount * next_state_values\n",
    "        q_targets = torch.from_numpy(q_targets).float().to(DEVICE)\n",
    "\n",
    "        # this selects only the q values of the actions taken\n",
    "        idx = row_idx, actions\n",
    "        states = torch.from_numpy(states).float().to(DEVICE)\n",
    "        action_values = self.q_net(states)[idx].float().to(DEVICE)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        td_error = self.criterion(q_targets, action_values)\n",
    "        td_error.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        else:\n",
    "            self.epsilon = self.epsilon_min\n",
    "\n",
    "#Execution du jeu\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    game = create_simple_game()\n",
    "    n = game.get_available_buttons_size()\n",
    "    actions = [list(a) for a in it.product([0, 1], repeat=n)]\n",
    "\n",
    "\n",
    "    agent = DQNAgent(len(actions), lr=learning_rate, batch_size=batch_size,\n",
    "                     memory_size=replay_memory_size, discount_factor=discount_factor,\n",
    "                     load_model=load_model)\n",
    "\n",
    "   \n",
    "    if not skip_learning:\n",
    "        agent, game = run(game, agent, actions, num_epochs=train_epochs, frame_repeat=frame_repeat,\n",
    "                          steps_per_epoch=learning_steps_per_epoch)\n",
    "\n",
    "        print(\"======================================\")\n",
    "        print(\"Training finished !\")\n",
    "\n",
    "   \n",
    "        # Sleep entre les runs\n",
    "        sleep(1.0)\n",
    "        score = game.get_total_reward()\n",
    "        print(\"Total score: \", score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
